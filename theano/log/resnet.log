[global]
device = gpu
floatX = float32
optimizer_including = unsafe

[cuda]
root = /usr/local/cuda-7.5/

[lib]
cnmem = 0.45

[dnn.conv]
algo_fwd = time_once
algo_bwd_filter = time_once
algo_bwd_data = time_once
Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 45.0% of memory, cuDNN 5103)
/usr/local/lib/python2.7/dist-packages/theano/gradient.py:536: UserWarning: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: mean
  handle_disconnected(elem)
/usr/local/lib/python2.7/dist-packages/theano/gradient.py:536: UserWarning: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: inv_std
  handle_disconnected(elem)
/usr/local/lib/python2.7/dist-packages/theano/gradient.py:562: UserWarning: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: <DisconnectedType>
  handle_disconnected(rval[i])
Building model...
number of parameters in model: 820152
Compiling theano functions...
Functions are compiled
2016-09-13 00:47:23.316236: step 10, duration = 0.199
2016-09-13 00:47:25.310696: step 20, duration = 0.199
2016-09-13 00:47:27.305227: step 30, duration = 0.199
2016-09-13 00:47:29.300375: step 40, duration = 0.200
2016-09-13 00:47:31.295895: step 50, duration = 0.199
2016-09-13 00:47:33.291479: step 60, duration = 0.200
2016-09-13 00:47:35.287301: step 70, duration = 0.200
2016-09-13 00:47:37.283801: step 80, duration = 0.200
2016-09-13 00:47:39.280390: step 90, duration = 0.200
2016-09-13 00:47:41.077291: Forward across 100 steps, 0.200 +/- 0.000 sec / batch
2016-09-13 00:48:02.865489: step 10, duration = 0.835
2016-09-13 00:48:11.219439: step 20, duration = 0.836
2016-09-13 00:48:19.575116: step 30, duration = 0.835
2016-09-13 00:48:27.989141: step 40, duration = 0.842
2016-09-13 00:48:36.402744: step 50, duration = 0.841
2016-09-13 00:48:44.818942: step 60, duration = 0.842
2016-09-13 00:48:53.235987: step 70, duration = 0.842
2016-09-13 00:49:01.654908: step 80, duration = 0.842
2016-09-13 00:49:10.075503: step 90, duration = 0.842
2016-09-13 00:49:17.655441: Forward-Backward across 100 steps, 0.840 +/- 0.003 sec / batch
