I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.7.5 locally
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:05:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x2a1b600
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:08:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:08:00.0)
2016-09-12 12:11:15.818521: step 10, duration = 0.028
2016-09-12 12:11:16.103673: step 20, duration = 0.028
2016-09-12 12:11:16.389514: step 30, duration = 0.029
2016-09-12 12:11:16.675671: step 40, duration = 0.028
2016-09-12 12:11:16.961612: step 50, duration = 0.028
2016-09-12 12:11:17.246977: step 60, duration = 0.028
2016-09-12 12:11:17.532660: step 70, duration = 0.029
2016-09-12 12:11:17.818818: step 80, duration = 0.029
2016-09-12 12:11:18.104850: step 90, duration = 0.029
2016-09-12 12:11:18.390743: step 100, duration = 0.029
2016-09-12 12:11:18.677898: step 110, duration = 0.029
2016-09-12 12:11:18.964840: step 120, duration = 0.029
2016-09-12 12:11:19.251336: step 130, duration = 0.029
2016-09-12 12:11:19.538252: step 140, duration = 0.029
2016-09-12 12:11:19.825838: step 150, duration = 0.029
2016-09-12 12:11:20.113200: step 160, duration = 0.029
2016-09-12 12:11:20.400482: step 170, duration = 0.029
2016-09-12 12:11:20.687766: step 180, duration = 0.029
2016-09-12 12:11:20.974995: step 190, duration = 0.029
2016-09-12 12:11:21.262160: step 200, duration = 0.029
2016-09-12 12:11:21.549126: step 210, duration = 0.029
2016-09-12 12:11:21.836916: step 220, duration = 0.029
2016-09-12 12:11:22.124078: step 230, duration = 0.029
2016-09-12 12:11:22.411653: step 240, duration = 0.029
2016-09-12 12:11:22.698790: step 250, duration = 0.029
2016-09-12 12:11:22.986503: step 260, duration = 0.029
2016-09-12 12:11:23.273867: step 270, duration = 0.029
2016-09-12 12:11:23.561274: step 280, duration = 0.029
2016-09-12 12:11:23.847608: step 290, duration = 0.029
2016-09-12 12:11:24.134602: step 300, duration = 0.029
2016-09-12 12:11:24.421548: step 310, duration = 0.029
2016-09-12 12:11:24.708397: step 320, duration = 0.029
2016-09-12 12:11:24.994370: step 330, duration = 0.028
2016-09-12 12:11:25.281367: step 340, duration = 0.029
2016-09-12 12:11:25.567042: step 350, duration = 0.028
2016-09-12 12:11:25.853352: step 360, duration = 0.029
2016-09-12 12:11:26.140153: step 370, duration = 0.029
2016-09-12 12:11:26.426834: step 380, duration = 0.029
2016-09-12 12:11:26.712314: step 390, duration = 0.029
fake 2016-09-12 12:11:26.969843: Forward-backward across 400 steps, 0.029 +/- 0.001 sec / batch
